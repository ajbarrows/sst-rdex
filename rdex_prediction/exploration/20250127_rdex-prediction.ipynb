{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import glob\n",
    "\n",
    "from itertools import repeat\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import BPt as bp\n",
    "from BPt.extensions import LinearResidualizer\n",
    "\n",
    "from joblib import Parallel, delayed    \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from matplotlib.colors import Normalize\n",
    "from matplotlib.colorbar import make_axes\n",
    "from matplotlib.cm import ScalarMappable\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from abcd_tools.utils.io import load_tabular\n",
    "from abcd_tools.utils.ConfigLoader import load_yaml\n",
    "from abcd_tools.image.preprocess import map_hemisphere\n",
    "\n",
    "import BPt as bp\n",
    "\n",
    "import neurotools.plotting as ntp\n",
    "from neurotools.plotting.ref import SurfRef, VolRef\n",
    "\n",
    "from nilearn.datasets import fetch_atlas_surf_destrieux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'ridge'\n",
    "summary = pd.read_csv(params['model_results_path'] + f'{model}_models_summary.csv')\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = load_yaml(\"../parameters.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_betas(betas_path: str) -> pd.DataFrame:\n",
    "    \"\"\"Load partitioned betas.\n",
    "\n",
    "    Args:\n",
    "        betas_path (str): Path to partitioned betas.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Partitioned betas.\n",
    "    \"\"\"\n",
    "    files = os.listdir(betas_path)\n",
    "    df = pd.DataFrame()\n",
    "    for f in files:\n",
    "        tmp = pd.read_parquet(betas_path + f)\n",
    "        df = pd.concat([df, tmp], axis=1)\n",
    "    return df\n",
    "# betas = load_betas(params[\"betas_path\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mri_confounds_path = params['mri_confounds_path']\n",
    "# mri_confounds = load_tabular(mri_confounds_path)\n",
    "# mri_confounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# targets_path = params['filtered_behavioral_path']\n",
    "# targets = load_tabular(targets_path)\n",
    "# targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_scopes(betas: pd.DataFrame, mri_confounds: pd.DataFrame, \n",
    "        fpath: str=None) -> dict:\n",
    "    \"\"\"Assign names to predictors based on SST condition, plus covariates.\n",
    "\n",
    "    Args:\n",
    "        betas (pd.DataFrame): Concatenated betas. \n",
    "        mri_confounds (pd.DataFrame): MRI confounds.  \n",
    "\n",
    "    Returns:\n",
    "        dict: Scopes\n",
    "    \"\"\"\n",
    "    unique_regressors = set([c.rsplit('_', 2)[0] for c in betas.columns])\n",
    "    scopes = {}\n",
    "\n",
    "    for u in unique_regressors:\n",
    "        scopes[u] = []\n",
    "        for c in betas.columns:\n",
    "            if u == '_'.join(c.split('_', 2)[:2]):\n",
    "                scopes[u].append(c)\n",
    "\n",
    "    scopes['mri_confounds'] = mri_confounds.columns\n",
    "\n",
    "    if fpath is not None:\n",
    "        pd.to_pickle(scopes, fpath)\n",
    "        print(f\"Scopes saved to {fpath}\")\n",
    "    \n",
    "    return scopes\n",
    "\n",
    "\n",
    "# predictor_scopes = gather_scopes(betas, mri_confounds, params['model_results_path'])\n",
    "# predictor_scopes.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bpt_dataset(betas: pd.DataFrame, scopes: dict, mri_confounds: pd.DataFrame, \n",
    "                targets: pd.DataFrame, test_split=0.2, random_state=42,\n",
    "                fpath: str=None) -> bp.Dataset:\n",
    "    \"\"\"Create a BPt dataset from betas, confounds, and targets.\n",
    "\n",
    "    Args:\n",
    "        betas (pd.DataFrame): Concatenated betas.\n",
    "        scopes (dict): Scopes.\n",
    "        mri_confounds (pd.DataFrame): MRI confounds.\n",
    "        targets (pd.DataFrame): Behavioral targets.\n",
    "        test_split (float, optional): Test split. Defaults to 0.2.\n",
    "        random_state (int, optional): Random state. Defaults to 42.\n",
    "        fpath (str, optional): Path to save dataset. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        bp.Dataset: BPt dataset.\n",
    "    \"\"\"\n",
    "    \n",
    "    scopes['covariates'] = mri_confounds.columns.tolist()\n",
    "\n",
    "    df = pd.concat([betas, mri_confounds, targets], axis=1)\n",
    "    dataset = bp.Dataset(df, targets=targets.columns.tolist())\n",
    "\n",
    "    for k, v in scopes.items():\n",
    "        dataset.add_scope(v, k, inplace=True)\n",
    "\n",
    "    dataset = dataset.auto_detect_categorical()\n",
    "    dataset = dataset.add_scope('mri_info_deviceserialnumber', 'category')\n",
    "    dataset = dataset.ordinalize('category')\n",
    "\n",
    "    dataset = dataset.dropna()\n",
    "\n",
    "    dataset = dataset.set_test_split(test_split, random_state=random_state)\n",
    "\n",
    "    if fpath is not None:\n",
    "        dataset.to_pickle(fpath)\n",
    "        print(f\"Dataset saved to {fpath}\")\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "# ds = make_bpt_dataset(betas, predictor_scopes, mri_confounds, targets)\n",
    "\n",
    "# ds.to_pickle(\"../../data/04_model_input/rdex_prediction_dataset.pkl\")\n",
    "# ds = pd.read_pickle(\"../../data/04_model_input/rdex_prediction_dataset.pkl\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_regression_pipeline(ds: bp.Dataset, scopes: dict, \n",
    "        model: str) -> bp.Pipeline:\n",
    "\n",
    "    # Just scale float type features\n",
    "    robust_scaler = bp.Scaler('robust', scope='float')\n",
    "\n",
    "    # Define residualization procedure\n",
    "    ohe = OneHotEncoder(categories='auto', drop='if_binary', \n",
    "            sparse=False, handle_unknown='ignore')\n",
    "    ohe_tr = bp.Transformer(ohe, scope='category')\n",
    "    \n",
    "    resid = LinearResidualizer(to_resid_df=ds['covariates'], fit_intercept=True)\n",
    "    resid_tr = bp.Scaler(resid, scope=list(scopes.keys()))\n",
    "\n",
    "    # Define regression model\n",
    "    mod_params = {'alpha': bp.p.Log(lower=1e-5, upper=1e5)}\n",
    "\n",
    "    if model == \"ridge\":\n",
    "        mod_obj=\"ridge\"\n",
    "    elif model == 'elastic':\n",
    "        mod_obj=ElasticNet()\n",
    "        l1_ratio = bp.p.Scalar(lower=0.001, upper=1).set_mutation(sigma=0.165)\n",
    "        mod_params['l1_ratio'] = l1_ratio\n",
    "    elif model == 'lasso':\n",
    "        mod_obj='lasso regressor'\n",
    "\n",
    "    param_search = bp.ParamSearch('HammersleySearch', n_iter=100, cv='default')\n",
    "    model = bp.Model(\n",
    "        obj=mod_obj, \n",
    "        params=mod_params,  \n",
    "        param_search=param_search\n",
    "    )\n",
    "\n",
    "    return bp.Pipeline([robust_scaler, ohe_tr, resid_tr, model])\n",
    "\n",
    "# pipe = define_regression_pipeline(ds, predictor_scopes, 'ridge')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(ds: bp.Dataset, scopes: dict, model='ridge', complete=False,\n",
    "    n_cores=-1, random_state=42) -> bp.CompareDict:\n",
    "    \n",
    "    if model in ['elastic', 'ridge', 'lasso']:\n",
    "        pipe = define_regression_pipeline(ds, scopes, model=model)\n",
    "    else:\n",
    "        raise Exception(f'Specified model {model} is not implemented')\n",
    "    \n",
    "    if not complete:\n",
    "        compare_scopes = []\n",
    "        for key in scopes.keys():\n",
    "            if key != 'covariates': \n",
    "                compare_scopes.append(bp.Option(['covariates', key], name=f'cov + {key}'))\n",
    "            else:\n",
    "                compare_scopes.append('covariates')\n",
    "        compare_scopes = bp.Compare(compare_scopes)\n",
    "    else:\n",
    "        compare_scopes = None\n",
    "\n",
    "    if n_cores == -1:\n",
    "        n_cores = os.cpu_count()\n",
    "\n",
    "    ps = bp.ProblemSpec(n_jobs=n_cores, random_state=random_state)\n",
    "    cv = bp.CV(splits=5, n_repeats=1)\n",
    "\n",
    "    results = bp.evaluate(pipeline=pipe,\n",
    "                      dataset=ds,\n",
    "                      problem_spec=ps,\n",
    "                      scope=compare_scopes,\n",
    "                      mute_warnings=True,\n",
    "                      target=bp.Compare(ds.get_cols('target')),\n",
    "                      cv=cv)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_scopes(keys: list, scopes: dict) -> dict:\n",
    "    keys.extend(['mri_confounds'])\n",
    "    return {k: v for k, v in scopes.items() if k in keys}\n",
    "    \n",
    "def save_model_results(res: bp.CompareDict, name: str, model: str, path: str) -> None:\n",
    "    \"\"\"Save model results to disk.\n",
    "\n",
    "    Args:\n",
    "        res (bp.CompareDict): Model results.\n",
    "        name (str): Model name.\n",
    "        model (str): Model type.\n",
    "        path (str): Path to save results.\n",
    "    \"\"\"\n",
    "    summary = res.summary()\n",
    "    summary.to_csv(f'{path}/{name}_{model}_summary.csv')\n",
    "    pd.to_pickle(res, f'{path}/{name}_{model}_results.pkl')\n",
    "    print(f\"Results saved to {path}\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_full_model(ds: bp.Dataset, scopes: dict, model: str, fpath: str) -> bp.CompareDict:\n",
    "    res = fit_model(ds, scopes, model, complete=True)\n",
    "    save_model_results(res, 'all_conditions', model, fpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_comparisons = ['ridge', 'elastic', 'lasso']\n",
    "predictor_comparison = ['incorrect_go', 'correct_go', 'correct_stop, incorrect_stop']\n",
    "\n",
    "\n",
    "# scopes = filter_scopes(['incorrect_go'], predictor_scopes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_slurm_script(model: str, \n",
    "                    condition: str,\n",
    "                    walltime: str='24:00:00',\n",
    "                    mem: str='256G', \n",
    "                    envname: str='sst-rdex',\n",
    "                    fpath:str=\"../slurm/\") -> str:\n",
    "    script = f\"\"\"#!/bin/bash\n",
    "#SBATCH --job-name={model}_{condition}\n",
    "#SBATCH --output=logs/%x_%j.out\n",
    "#SBATCH --error=logs/%x_%j.err\n",
    "#SBATCH --time={walltime}\n",
    "#SBATCH --partition=general\n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --cpus-per-task=8\n",
    "#SBATCH --mem={mem}\n",
    "#SBATCH --mail-type=ALL\n",
    "\n",
    "source ${{HOME}}/.bashrc\n",
    "conda activate {envname}\n",
    "\n",
    "cd ../pipelines/\n",
    "\n",
    "python3 run_model.py {model} {condition}\n",
    "\"\"\"\n",
    "    with open(f\"{fpath}/{model}_{condition}.sh\", 'w') as f:\n",
    "        f.write(script)\n",
    "    print(f\"Slurm script saved to {fpath}/{model}_{condition}.sh\")\n",
    "    \n",
    "    return script\n",
    "\n",
    "\n",
    "# model_comparisons = ['ridge', 'elastic', 'lasso']\n",
    "# predictor_comparison = ['incorrect_go', 'correct_go', 'correct_stop', 'incorrect_stop']\n",
    "\n",
    "# for model in model_comparisons:\n",
    "#     for condition in predictor_comparison:\n",
    "#         generate_slurm_script(model, condition)\n",
    "\n",
    "\n",
    "# generate_slurm_script('ridge', 'incorrect_go')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def make_effect_compare_plot(df, title='Vertexwise Regressor Model Fit Comparison', contrasts=False):\n",
    "\n",
    "#     if contrasts:\n",
    "#         df['procedure'] = df['procedure'].str.cat(df['inputs'], sep=': ')\n",
    "#         hatches = ['', 'xx', 'oo', 'OO']\n",
    "#     else:\n",
    "#         hatches = ['', 'oo', '+', 'OO']\n",
    "\n",
    "#     fig, ax = plt.subplots(figsize=(15,5))\n",
    "    \n",
    "#     # greypallete = np.repeat('lightgrey', len(df))\n",
    "\n",
    "#     n_procedures = len(df['procedure'].drop_duplicates())\n",
    "#     greypallete = list(np.repeat('lightgrey', n_procedures))\n",
    "\n",
    "#     order =df['target'].drop_duplicates()\n",
    "\n",
    "#     g = sns.barplot(\n",
    "#         x='target', \n",
    "#         y='mean_scores_r2', \n",
    "#         hue='procedure', \n",
    "#         data=df,\n",
    "#         palette=greypallete,\n",
    "#         order=order)\n",
    "\n",
    "#     g.legend_.set_title('')\n",
    "\n",
    "#     ax.grid(linestyle=':')\n",
    "#     bars = ax.patches[:len(ax.patches)-n_procedures]\n",
    "#     x_coords = [p.get_x() + 0.5 * p.get_width() for p in bars]\n",
    "#     y_coords = [p.get_height() for p in bars]\n",
    "\n",
    "#     ax.errorbar(x=x_coords, y=y_coords, yerr=df[\"std_scores_r2\"], fmt=\"none\", c=\"k\")\n",
    "\n",
    "#     # only want one set of colors\n",
    "#     palette = df[['target', 'color']].drop_duplicates()['color']\n",
    " \n",
    "#     for bars, hatch, legend_handle in zip(ax.containers, hatches, \n",
    "#                                           ax.legend_.legendHandles):\n",
    "#         for bar, color in zip(bars, palette):\n",
    "#             bar.set_facecolor(color)\n",
    "#             bar.set_hatch(hatch)\n",
    "#         legend_handle.set_hatch(hatch + hatch)\n",
    "\n",
    "#     ptplt = sns.pointplot(\n",
    "#         x='target', \n",
    "#         y='test_r2', \n",
    "#         data=df, \n",
    "#         hue='procedure', \n",
    "#         markersize=5,\n",
    "#         dodge=0.5, \n",
    "#         linestyles=\"none\",\n",
    "#         palette=greypallete,\n",
    "#         order=order,\n",
    "#         legend=False\n",
    "#     )\n",
    "\n",
    "    # # formatting\n",
    "    # ax.set(xlabel=None)\n",
    "    # ax.set(ylabel='Avg. $R^{2}$')\n",
    "    # ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "    # ax.spines[['top', 'right']].set_visible(False)\n",
    "\n",
    "    # fig.subplots_adjust(top=0.9)\n",
    "    # fig.suptitle(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_test_prediction(results_summary: pd.DataFrame, test_prediction: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Join test predictions with results summary.\n",
    "\n",
    "    Args:\n",
    "        results_summary (pd.DataFrame): Results summary.\n",
    "        test_prediction (pd.DataFrame): Test predictions.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Joined dataframes.\n",
    "    \"\"\"\n",
    "    idx = ['target', 'scope']\n",
    "    return results_summary.join(test_prediction.set_index(idx), on=idx, how='left')\n",
    "\n",
    "def get_test_prediction(results, metric='r2'):\n",
    "    \"\"\"Get test prediction from results.\n",
    "\n",
    "    Args:\n",
    "        results (bp.CompareDict): Model results.\n",
    "        metric (str, optional): Metric to extract. Defaults to 'r2'.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Test predictions\n",
    "    \"\"\"\n",
    "\n",
    "    if isinstance(results, str):\n",
    "        results = pd.read_pickle(results)\n",
    "\n",
    "    scores = pd.DataFrame()\n",
    "\n",
    "    for l, m in results.items():\n",
    "\n",
    "        lab = l.__dict__['options']\n",
    "\n",
    "        if len(lab) == 1:\n",
    "            scope = 'all'\n",
    "            target = lab[0].__dict__['name']\n",
    "        else:\n",
    "            scope = lab[0].__dict__['name']\n",
    "            target = lab[1].__dict__['name']\n",
    "\n",
    "        if scope == 'cov + mri_confounds':\n",
    "            continue\n",
    "        else:\n",
    "\n",
    "            best_model_idx = np.argmax(m.scores[metric])\n",
    "            best_model = m.estimators[best_model_idx]\n",
    "\n",
    "            ds = m._dataset\n",
    "            X_test, y_test = ds.get_Xy(m.ps, subjects='test')\n",
    "            pred = best_model.score(X_test, y_test)\n",
    "            \n",
    "            tmp = pd.DataFrame({'scope': scope, 'target': target, 'test_r2': pred}, index=[0])\n",
    "            scores = pd.concat([scores, tmp], axis=0)\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assemble_summary(results_path: str, params: dict, model='ridge', n_jobs=2) -> pd.DataFrame:\n",
    "    \"\"\"Assemble model summary.\n",
    "    \n",
    "    Args:\n",
    "        results_path (str): Path to model results.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Model summary.\n",
    "    \"\"\"\n",
    "\n",
    "    results_paths = glob.glob(params['model_results_path']+ f\"*{model}_results.pkl\")\n",
    "    summary_paths = glob.glob(params['model_results_path']+ f\"*{model}_summary.csv\")\n",
    "\n",
    "    test_predictions = Parallel(n_jobs=n_jobs)(\n",
    "        delayed(get_test_prediction)(r) for r in results_paths\n",
    "    )\n",
    "    summary = pd.concat([pd.read_csv(p) for p in summary_paths])\n",
    "    summary = join_test_prediction(summary, pd.concat(test_predictions))\n",
    "\n",
    "    summary.to_csv(results_path + f'{model}_models_summary.csv')\n",
    "\n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relabel_plotting_data(df, process_map, target_map, color_map):\n",
    "  \"\"\"Relabel data for plotting.\n",
    "\n",
    "  Args:\n",
    "      df (pd.DataFrame): Dataframe.\n",
    "      process_map (dict): Process map.\n",
    "      target_map (dict): Target map.\n",
    "      color_map (dict): Color map.\n",
    "\n",
    "  Returns:\n",
    "      pd.DataFrame: Relabeled dataframe\n",
    "  \"\"\"\n",
    "\n",
    "  df = df[df['scope'] != 'cov + mri_confounds']\n",
    "  df['scope'] = df['scope'].str.replace('cov \\+ ', '', regex=True)\n",
    "  df.loc[:, 'process'] = df['target']\n",
    "  df['process'] = df['process'].replace(process_map)\n",
    "\n",
    "  df.loc[:, 'color'] = df['process']\n",
    "  df['color'] = df['color'].replace(color_map)\n",
    "  df['target'] = df['target'].replace(target_map)\n",
    "\n",
    "  return df\n",
    "\n",
    "def sort(df: pd.DataFrame) -> pd.DataFrame:\n",
    "  \"\"\"Sort dataframe.\n",
    "\n",
    "  Args:\n",
    "      df (pd.DataFrame): Dataframe.\n",
    "    \n",
    "  Returns:\n",
    "      pd.DataFrame: Sorted dataframe.\n",
    "  \"\"\"\n",
    "  avg = (df[['target', 'mean_scores_r2', 'std_scores_r2']]\n",
    "          .groupby('target')\n",
    "          .mean(numeric_only=True)\n",
    "          .sort_values('mean_scores_r2', ascending=False)\n",
    "  )\n",
    "  avg.columns = ['avg_mean', 'avg_std']\n",
    "  df = (df\n",
    "        .set_index('target')\n",
    "      #   .drop(columns=['test_r2'])\n",
    "        .join(avg)\n",
    "        .sort_values(by=['process', 'avg_mean'], ascending=[True, False])\n",
    "        .reset_index()\n",
    "        .drop(columns=['avg_mean', 'avg_std'])\n",
    "  )\n",
    "  return df\n",
    "\n",
    "\n",
    "process_map = params['process_map']\n",
    "target_map = params['target_map']\n",
    "color_map = params['color_map']\n",
    "\n",
    "# summary = pd.read_csv(params['model_results_path'] + 'ridge_models_summary.csv')\n",
    "# summary = relabel_plotting_data(summary, process_map, target_map, color_map)\n",
    "# sort(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_effect_compare_plot(df, title='Vertexwise Regressor Model Fit Comparison'):\n",
    "\n",
    "    hatches = ['', '/', '-', 'X', 'O']\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(15,5))\n",
    "    \n",
    "    # greypallete = np.repeat('lightgrey', len(df))\n",
    "\n",
    "    n_scopes = len(df['scope'].drop_duplicates())\n",
    "    greypallete = list(np.repeat('lightgrey', n_scopes))\n",
    "\n",
    "    order =df['target'].drop_duplicates()\n",
    "\n",
    "    g = sns.barplot(\n",
    "        x='target', \n",
    "        y='mean_scores_r2', \n",
    "        hue='scope', \n",
    "        data=df,\n",
    "        palette=greypallete,\n",
    "        order=order)\n",
    "\n",
    "    g.legend_.set_title('')\n",
    "\n",
    "    ax.grid(linestyle=':')\n",
    "    bars = ax.patches[:len(ax.patches)-n_scopes]\n",
    "    x_coords = [p.get_x() + 0.5 * p.get_width() for p in bars]\n",
    "    y_coords = [p.get_height() for p in bars]\n",
    "\n",
    "    ax.errorbar(x=x_coords, y=y_coords, yerr=df[\"std_scores_r2\"], fmt=\"none\", c=\"k\")\n",
    "\n",
    "    # only want one set of colors\n",
    "    palette = df[['target', 'color']].drop_duplicates()['color']\n",
    " \n",
    "    for bars, hatch, legend_handle in zip(ax.containers, hatches, \n",
    "                                          ax.legend_.legendHandles):\n",
    "        for bar, color in zip(bars, palette):\n",
    "            bar.set_facecolor(color)\n",
    "            bar.set_hatch(hatch)\n",
    "        legend_handle.set_hatch(hatch + hatch)\n",
    "\n",
    "    ptplt = sns.pointplot(\n",
    "        x='target', \n",
    "        y='test_r2', \n",
    "        data=df, \n",
    "        hue='scope', \n",
    "        markersize=2,\n",
    "        dodge=0.5, \n",
    "        linestyles=\"none\",\n",
    "        palette=greypallete,\n",
    "        order=order,\n",
    "        legend=False\n",
    "    )\n",
    "\n",
    "    # formatting\n",
    "    ax.set(xlabel=None)\n",
    "    ax.set(ylabel='Avg. $R^{2}$')\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "    ax.spines[['top', 'right']].set_visible(False)\n",
    "\n",
    "    fig.subplots_adjust(top=0.9)\n",
    "    fig.suptitle(title)\n",
    "\n",
    "# make_effect_compare_plot(sort(summary), title='Vertexwise Regressor Model Fit Comparison')\n",
    "# plt.savefig(params['plot_output_path'] + 'vertexwise_regressor_model_fit_comparison_ridge.png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_paths = glob.glob(params['model_results_path']+ f\"*ridge_results.pkl\")\n",
    "results_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_nonfeatures(coefs: pd.Series, filter_strings = ['mri', 'iqc']) -> pd.Series:\n",
    "    return coefs[~coefs.index.str.contains('|'.join(filter_strings))]\n",
    "\n",
    "def get_feature_importance(results: bp.CompareDict, metric='r2'\n",
    "                                        ) -> (pd.DataFrame, pd.DataFrame, pd.DataFrame):\n",
    "    \"\"\"Get feature importance from results.\n",
    "    \n",
    "    Args:\n",
    "        results (bp.CompareDict): Model results.\n",
    "        metric (str, optional): Metric to extract. Defaults to 'r2'.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Feature importance.\n",
    "    \"\"\"\n",
    "    \n",
    "    fis = {}\n",
    "    best_fis = {}\n",
    "    avg_fis = {}\n",
    "\n",
    "    if isinstance(results, str):\n",
    "        results = pd.read_pickle(results)\n",
    "\n",
    "    for l, m in results.items():\n",
    "\n",
    "        lab = l.__dict__['options']\n",
    "\n",
    "        if len(lab) == 1:\n",
    "            scope = 'all'\n",
    "            target = lab[0].__dict__['name']\n",
    "        else:\n",
    "            scope = lab[0].__dict__['name']\n",
    "            scope = scope.replace('cov + ', '')\n",
    "            target = lab[1].__dict__['name']\n",
    "\n",
    "        if scope == 'mri_confounds':\n",
    "            continue\n",
    "        else:\n",
    "            coefs = m.get_fis()\n",
    "            fis[target] = coefs\n",
    "\n",
    "            avg_coefs = coefs.mean()\n",
    "            avg_coefs = remove_nonfeatures(avg_coefs)\n",
    "            avg_fis[target] = avg_coefs\n",
    "\n",
    "            best_model_idx = np.argmax(m.scores[metric])\n",
    "            best_coefs = coefs.iloc[best_model_idx, :]\n",
    "            best_coefs = remove_nonfeatures(best_coefs)\n",
    "            best_fis[target] = best_coefs\n",
    "\n",
    "    return fis, best_fis, avg_fis\n",
    "\n",
    "def assemble_feature_importance(fpath: str, params: dict, model: str='ridge', n_jobs=2) -> None:\n",
    "    \"\"\"Implement gather_feature_importance.\"\"\"\n",
    "\n",
    "    results_paths = glob.glob(params['model_results_path']+ f\"*ridge_results.pkl\")\n",
    "    res = Parallel(n_jobs=n_jobs)(\n",
    "        delayed(get_feature_importance)(r) for r in results_paths\n",
    "    )\n",
    "    pd.to_pickle(res, fpath + f'{model}_feature_importance.pkl')\n",
    "\n",
    "\n",
    "# assemble_feature_importance(params['model_results_path'], model='ridge')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def broadcast_to_fsaverage(fis_agg: pd.Series, n_vertices=10242+1) -> pd.DataFrame:\n",
    "\n",
    "    def _split_hemisphere(df):\n",
    "        df = df.reset_index(names=['correct', 'condition', 'hemisphere'])\n",
    "        lh = df[df['hemisphere'] == 'lh'].drop(columns='hemisphere')\n",
    "        rh = df[df['hemisphere'] == 'rh'].drop(columns='hemisphere')\n",
    "\n",
    "        # idx = ['correct', 'condition']\n",
    "        # return lh.set_index(idx), rh.set_index(idx)\n",
    "        return lh, rh\n",
    "\n",
    "    fis = fis_agg.copy()\n",
    "\n",
    "    fis.index = fis.index.str.split('_', expand=True)\n",
    "    fis = fis.unstack(level=2)\n",
    "    fis.columns = pd.to_numeric(fis.columns).sort_values()\n",
    "\n",
    "    # need to insert blank columns for missing vertices\n",
    "    vertex_names = [*range(1, n_vertices)]\n",
    "    null_df = pd.DataFrame(np.nan, columns=vertex_names, index=fis.index)\n",
    "    null_df = null_df.drop(columns=fis.columns)\n",
    "\n",
    "    df = fis.join(null_df, how='outer')\n",
    "    lh, rh = _split_hemisphere(df)\n",
    "\n",
    "\n",
    "    return lh, rh\n",
    "def load_destrieux_atlas():\n",
    "    atlas = fetch_atlas_surf_destrieux()\n",
    "    return atlas\n",
    "def map_destrieux(lh: pd.DataFrame, rh: pd.DataFrame, prefix: str='') -> pd.DataFrame:\n",
    "\n",
    "    dest = load_destrieux_atlas()\n",
    "\n",
    "    idx = ['correct', 'condition']\n",
    "    lh = lh.set_index(idx)\n",
    "    rh = rh.set_index(idx)\n",
    "\n",
    "    lh_mapped = map_hemisphere(lh, \n",
    "               mapping=dest['map_left'], \n",
    "               labels=dest['labels'],\n",
    "               prefix=prefix,\n",
    "               suffix='.lh')\n",
    "    rh_mapped = map_hemisphere(rh,\n",
    "                mapping=dest['map_right'],\n",
    "                labels=dest['labels'],\n",
    "                prefix=prefix,\n",
    "                suffix='.rh')\n",
    "\n",
    "    lh_mapped.index = lh.index\n",
    "    rh_mapped.index = rh.index\n",
    " \n",
    "    df = pd.concat([lh_mapped, rh_mapped], axis=1)\n",
    "    vmin, vmax = get_fullrang_minmax(df)\n",
    "\n",
    "    return lh_mapped.reset_index(), rh_mapped.reset_index(), vmin, vmax\n",
    "\n",
    "\n",
    "def absmax(x):\n",
    "    idx = np.argmax(np.abs(x))\n",
    "    return x[idx]\n",
    "\n",
    "\n",
    "def get_fullrang_minmax(series: pd.Series):\n",
    "    mi = series.min().min()\n",
    "    ma = series.max().max()\n",
    "\n",
    "    abs_max = max(np.abs(mi), np.abs(ma))\n",
    "\n",
    "    return -abs_max, abs_max\n",
    "\n",
    "\n",
    "def draw_plot(lh, rh, ax, mode, cmap='bwr', vmin=None, vmax=None, avg_method=absmax):\n",
    "\n",
    "    if mode == 'roi':\n",
    "        plot_df = pd.concat([lh, rh], axis=1)\n",
    "        dest = fetch_atlas_surf_destrieux()\n",
    "        surf_ref = SurfRef(space='fsaverage5', parc='destr')\n",
    "        to_plot = surf_ref.get_hemis_plot_vals(plot_df)\n",
    "\n",
    "        ntp.plot(to_plot, threshold=0, ax=ax, cmap=cmap,\n",
    "            vmin=vmin, vmax=vmax, colorbar=False)\n",
    "\n",
    "    else:\n",
    "        # to_plot = surf_ref.get_hemis_plot_vals(dest)\n",
    "        # ntp.plot(to_plot, threshold=0)\n",
    "        plot_dict = {'lh': lh.values, 'rh': rh.values}\n",
    "        ntp.plot(plot_dict, \n",
    "            avg_method=avg_method, \n",
    "            ax=ax, cmap=cmap, \n",
    "            vmin=vmin, vmax=vmax, \n",
    "            colorbar=False, \n",
    "            threshold=0)\n",
    "    \n",
    "def make_collage_plot(fis_agg: dict, target, target_map, basepath='../../data/06_reporting/rdex_prediction/fis_plots', \n",
    "    agg='avg_fis', mode='vertex', model='enet', fontsize=25) :\n",
    "\n",
    "    def _format_for_plotting(fis: pd.DataFrame, correct: str, condition: str) -> pd.DataFrame:\n",
    "        tmp = fis[(fis['correct'] == correct) & (fis['condition'] == condition)]\n",
    "\n",
    "        tmp = tmp.drop(columns=['correct', 'condition'])\n",
    "        tmp[np.isnan(tmp)] = 0\n",
    "        return tmp\n",
    "               \n",
    "    lh, rh = broadcast_to_fsaverage(fis_agg[target])\n",
    " \n",
    "\n",
    "    conditions = pd.unique(lh['condition'])\n",
    "    n_cond = conditions.shape[0]\n",
    "    directions = ['correct', '', 'incorrect']\n",
    "    width_ratios = [1]\n",
    "    height_ratios = [100, 1, 100]\n",
    "\n",
    "    col_ratios = list(repeat(10, n_cond))\n",
    "\n",
    "    width_ratios.extend(col_ratios)\n",
    "    width_ratios.extend([2]) # colorbar\n",
    "\n",
    "    gs = {\n",
    "        'width_ratios': width_ratios,\n",
    "        'height_ratios': height_ratios,\n",
    "        'hspace':0,\n",
    "        'wspace':0\n",
    "    }\n",
    "\n",
    "    cmap = 'bwr'\n",
    "    nb_ticks = 5\n",
    "    cbar_tick_format='%.2g'\n",
    "\n",
    "    fig, axs = plt.subplots(3, n_cond + 1+1, figsize = (35, 20) , gridspec_kw=gs)\n",
    "    \n",
    "    for i, direction in enumerate(directions):\n",
    "        ax = axs[i, 0]\n",
    "        ax.set_axis_off()\n",
    "        if i==1: \n",
    "            continue\n",
    "        else:\n",
    "            ax.text(0, .5, direction, fontsize=fontsize)\n",
    "\n",
    "    if mode == 'roi':\n",
    "        lh, rh, vmin, vmax = map_destrieux(lh, rh)\n",
    "    else:\n",
    "        vmin, vmax = get_fullrang_minmax(fis_agg[target])\n",
    "    \n",
    "    cnt = 1\n",
    "    for condition in conditions:\n",
    "        top = axs[0, cnt]\n",
    "        middle = axs[1, cnt]\n",
    "        bottom = axs[2, cnt]\n",
    "\n",
    "        lh_correct = _format_for_plotting(lh, 'correct', condition)\n",
    "        rh_correct = _format_for_plotting(rh, 'correct', condition)\n",
    "        draw_plot(lh_correct, rh_correct, top, mode, vmin=vmin, vmax=vmax)\n",
    "\n",
    "        middle.set_axis_off() # make blank space\n",
    "\n",
    "        lh_incorrect = _format_for_plotting(lh, 'incorrect', condition)\n",
    "        rh_incorrect = _format_for_plotting(rh, 'incorrect', condition)\n",
    "        draw_plot(lh_incorrect, rh_incorrect, bottom, mode, vmin=vmin, vmax=vmax)\n",
    "        \n",
    "        top.set_title(condition, fontsize=fontsize)\n",
    "        cnt += 1\n",
    "    \n",
    "    # plot colorbar\n",
    "    norm = Normalize(vmin=vmin, vmax=vmax)\n",
    "    proxy_mappable = ScalarMappable(norm=norm, cmap=cmap)\n",
    "    ticks = np.linspace(vmin, vmax, nb_ticks)\n",
    "\n",
    "    right = axs[:, n_cond + 1]\n",
    "    \n",
    "    for ax in right.flat:\n",
    "        ax.set_axis_off()\n",
    "        \n",
    "    cax, kw = make_axes(right, fraction=.5, shrink=0.5)\n",
    "    cbar = fig.colorbar(proxy_mappable, cax=cax, ticks=ticks,\n",
    "                        orientation='vertical', format=cbar_tick_format,\n",
    "                        ticklocation='left')\n",
    "    \n",
    "    cbar.set_label(label='Avg. Feature Imp.', fontsize=fontsize - 2)\n",
    "\n",
    "    prefix = 'Target: '\n",
    "    title = target_map[target]\n",
    "\n",
    "    fig.suptitle(prefix + title, fontsize=fontsize+2)\n",
    "\n",
    "    fpath = f'{basepath}/{agg}/{mode}/{model}'\n",
    "    if not os.path.exists(fpath):\n",
    "        os.makedirs(fpath)\n",
    "        \n",
    "    plt.savefig(f'{fpath}/{target}.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mode(fis_agg: dict, target_map: dict, agg: str, mode: str,  model: str,\n",
    "     basepath='./data/08_reporting/fis_plots'):\n",
    "\n",
    "    Parallel(n_jobs=8)(delayed(make_collage_plot)(\n",
    "        fis_agg, target, target_map, basepath=basepath, agg=agg, mode=mode, model=model) \n",
    "        for target in fis_agg.keys())\n",
    "\n",
    "def make_fis_plots(avg_fis, best_fis,  target_map, model='enet'):\n",
    "\n",
    "    modes = ['vertex', 'roi']\n",
    "\n",
    "    for mode in modes:\n",
    "        plot_mode(avg_fis, target_map, mode=mode, agg='avg_fis', model=model)\n",
    "        plot_mode(best_fis, target_map, mode=mode, agg='best_fis', model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fis = pd.read_pickle(params['model_results_path'] + 'ridge_feature_importance.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_fis(fis: list):\n",
    "    \"\"\"Gather feature importance for plotting.\"\"\"\n",
    "\n",
    "    targets = fis[0][1].keys()\n",
    "    best_fis = {}\n",
    "    avg_fis = {}\n",
    "\n",
    "    for target in targets:\n",
    "        best_fis[target] = pd.concat([f[1][target] for f in fis])\n",
    "        avg_fis[target] = pd.concat([f[2][target] for f in fis])\n",
    "\n",
    "return best_fis, avg_fis\n",
    "\n",
    "\n",
    "# avg_fis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'EEA'\n",
    "\n",
    "make_collage_plot(avg_fis, target, target_map, agg='avg_fis', mode='roi', model='ridge')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
