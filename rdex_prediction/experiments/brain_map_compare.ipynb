{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import neurotools.plotting as ntp\n",
    "\n",
    "from abcd_tools.utils.ConfigLoader import load_yaml\n",
    "params = load_yaml('../parameters.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def broadcast_to_fsaverage(fis_agg: pd.Series, n_vertices=10242) -> pd.DataFrame:\n",
    "    \"\"\"Broadcast feature importance to fsaverage5.\n",
    "\n",
    "    Args:\n",
    "        fis_agg (pd.Series): Feature importance.\n",
    "        n_vertices (int, optional): Number of vertices. Defaults to 10242+1.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Broadcasted feature importance.\n",
    "    \"\"\"\n",
    "\n",
    "    def _split_hemisphere(df):\n",
    "        df = df.reset_index(names=[\"correct\", \"condition\", \"hemisphere\"])\n",
    "        lh = df[df[\"hemisphere\"] == \"lh\"].drop(columns=\"hemisphere\")\n",
    "        rh = df[df[\"hemisphere\"] == \"rh\"].drop(columns=\"hemisphere\")\n",
    "\n",
    "        return lh, rh\n",
    "\n",
    "    fis = fis_agg.copy()\n",
    "\n",
    "    fis.index = fis.index.str.split(\"_\", expand=True)\n",
    "    fis = fis.unstack(level=2)\n",
    "    # fis = fis.unstack()\n",
    "\n",
    "    # convert columns to integers and sort\n",
    "    fis.columns = fis.columns.astype(int)\n",
    "    fis = fis.reindex(sorted(fis.columns), axis=1)\n",
    "\n",
    "    # need to insert blank columns for missing vertices\n",
    "    vertex_names = [*range(1, n_vertices + 1)]\n",
    "    # vertex_names = [*range(0, n_vertices)]\n",
    "    null_df = pd.DataFrame(np.nan, columns=vertex_names, index=fis.index)\n",
    "    null_df = null_df.drop(columns=fis.columns)\n",
    "\n",
    "    df = fis.join(null_df, how=\"outer\")\n",
    "    lh, rh = _split_hemisphere(df)\n",
    "\n",
    "    return lh, rh\n",
    "\n",
    "def table_to_dict(df: pd.DataFrame, idx=['correct', 'condition']):\n",
    "    \"\"\"Take dataframe (hemi) where each row is a double-index condition\n",
    "    and return a dictionary of numpy arrays. \"\"\"\n",
    "\n",
    "    return (df\n",
    "        .assign(cond=lambda x: x[idx[0]] + '_' + x[idx[1]])\n",
    "        .fillna(0)\n",
    "        .drop(columns=idx)\n",
    "        .set_index('cond')\n",
    "        .groupby(level=0)\n",
    "        .apply(lambda x: x.values.flatten())\n",
    "        .to_dict()\n",
    "    )\n",
    "\n",
    "def apply_mask(hemi_dict, hemi_mask):\n",
    "\n",
    "    masked = {}\n",
    "    for condition, values in hemi_dict.items():\n",
    "        \n",
    "        masked[condition] = np.where(hemi_mask, values, 0)\n",
    "    \n",
    "    return masked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_features(fis, filter = 'incorrect',\n",
    "    parameters = ['EEA', 'tf', 'SSRT', 'B']):\n",
    "\n",
    "    res = {}\n",
    "\n",
    "    for parameter in parameters:\n",
    "\n",
    "        lh, rh = (\n",
    "            fis[parameter]\n",
    "            .filter(like=filter)\n",
    "            .pipe(broadcast_to_fsaverage)\n",
    "        )\n",
    "\n",
    "        res[parameter] = (\n",
    "            {\n",
    "                'lh': table_to_dict(lh),\n",
    "                'rh': table_to_dict(rh)\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_fis_path = params['model_results_path'] + 'ridge_feature_importance.pkl'\n",
    "lasso_fis_path = params['model_results_path'] + 'lasso_feature_importance.pkl'\n",
    "\n",
    "# ridge_fis = pd.read_pickle(ridge_fis_path)\n",
    "# lasso_fis = pd.read_pickle(lasso_fis_path)\n",
    "\n",
    "ridge_fis, ridge_best_fis, ridge_avg_fis, ridge_haufe_avg = pd.read_pickle(ridge_fis_path)\n",
    "lasso_fis, lasso_best_fis, lasso_avg_fis, lasso_haufe_avg = pd.read_pickle(lasso_fis_path)\n",
    "\n",
    "target_map = params['target_map']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data = {\n",
    "    'ridge': {\n",
    "        'avg': collect_features(ridge_avg_fis),\n",
    "        'haufe': collect_features(ridge_haufe_avg)\n",
    "    },\n",
    "    'lasso': {\n",
    "        'avg': collect_features(lasso_avg_fis),\n",
    "        'haufe': collect_features(lasso_haufe_avg)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_global_minmax(plot_data):\n",
    "    all_values = []\n",
    "    for method in plot_data.values():\n",
    "        for feature_dict in method.values():\n",
    "            for hemi in feature_dict.values():\n",
    "                all_values.extend(hemi.values())\n",
    "    \n",
    "    all_values = np.array(all_values)\n",
    "    all_values = all_values[~np.isnan(all_values)]\n",
    "    \n",
    "    min_val = all_values.min()\n",
    "    max_val = all_values.max()\n",
    "    return max(abs(min_val), abs(max_val))\n",
    "    \n",
    "\n",
    "def make_error_plot(plot_data):\n",
    "\n",
    "    aggregation = ['Average FIS', \"Haufe-Transformed FIS\"]\n",
    "    aggregation = {\n",
    "        'avg': 'Average FIS',\n",
    "        'haufe': \"Haufe-Transformed FIS\"\n",
    "    }\n",
    "\n",
    "    targets = ['EEA', 'tf', 'SSRT', 'B']\n",
    "\n",
    "    conditions = ['incorrect_go', 'incorrect_stop']\n",
    "    # models = ['ridge', 'lasso']\n",
    "\n",
    "    cond_mod = list(product(aggregation.keys(), conditions))\n",
    "    cond_mod = [('', '')] + cond_mod\n",
    "    targets = [''] + targets\n",
    "\n",
    "    print(list(cond_mod))\n",
    "    fig, axs = plt.subplots(ncols=5, nrows=len(targets), figsize=(25,15))\n",
    "    fontsize = 15\n",
    "\n",
    "    max = get_global_minmax(plot_data)\n",
    "    min = -max\n",
    "\n",
    "    axs[0, 0].set_axis_off()\n",
    "\n",
    "    for i, target in enumerate(targets):\n",
    "        \n",
    "        for j, cond in enumerate(cond_mod):\n",
    "\n",
    "            # column labels\n",
    "            if i == 0:\n",
    "                if cond == ('', ''):\n",
    "                    pass\n",
    "                else:\n",
    "                    ax = axs[0, j]\n",
    "                    ax.set_axis_off()\n",
    "                    text = cond[0].title() + '\\n\\n' +  cond[1].replace('_', ' ').title()\n",
    "                    ax.text(0, 0.5, text, fontsize=fontsize)\n",
    "            if j == 0:\n",
    "                # row labels\n",
    "                if target == '':\n",
    "                    pass\n",
    "                else:\n",
    "                    ax = axs[i, j]\n",
    "                    ax.set_axis_off()\n",
    "                    ax.text(0, 0.5, target_map[target], fontsize=fontsize)\n",
    "            if (i > 0) and (j > 0):\n",
    "\n",
    "                ax = axs[i, j]\n",
    "\n",
    "\n",
    "                data = plot_data[cond[0]][target]\n",
    "                lh = data['lh'][cond[1]]\n",
    "                rh = data['rh'][cond[1]]\n",
    "\n",
    "                ntp.plot(\n",
    "                    {'lh': lh, 'rh': rh},\n",
    "                    threshold=0,\n",
    "                    cmap='seismic',\n",
    "                    # colorbar=False,\n",
    "                    # vmin=min,\n",
    "                    # vmax=max,\n",
    "                    ax=ax\n",
    "                )\n",
    "   \n",
    "make_error_plot(plot_data['ridge'])\n",
    "plt.savefig(params['plot_output_path'] + 'ridge_error_fis_plot.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "make_error_plot(plot_data['lasso'])\n",
    "plt.savefig(params['plot_output_path'] + 'lasso_error_fis_plot.png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "contrast_fis_path = params['model_results_path'] + 'contrasts_ridge_feature_importance.pkl'\n",
    "\n",
    "contrast_fis, contrast_best_fis, contrast_avg_fis, contrast_haufe_avg = pd.read_pickle(contrast_fis_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_global_contrast_minmax(plot_data):\n",
    "    all_values = []\n",
    "    for target in plot_data.values():\n",
    "        for hemi in target.values():\n",
    "            for condition in hemi.values():\n",
    "                all_values.extend(condition)\n",
    "    \n",
    "    all_values = np.array(all_values)\n",
    "    all_values = all_values[~np.isnan(all_values)]\n",
    "    \n",
    "    min_val = all_values.min()\n",
    "    max_val = all_values.max()\n",
    "    return max(abs(min_val), abs(max_val))\n",
    "\n",
    "contrasts_plot = collect_features(\n",
    "    contrast_haufe_avg,\n",
    "    filter='correct'\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_global_contrast_minmax(contrasts_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import Normalize\n",
    "from matplotlib.cm import ScalarMappable\n",
    "from matplotlib.colorbar import make_axes\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "def make_colorbar(\n",
    "    fig, ax, vmin, vmax, cmap, label=\"Haufe-Transformed Feature Importance\"\n",
    "):\n",
    "\n",
    "    # plot colorbar\n",
    "    nb_ticks = 5\n",
    "    cbar_tick_format = \"%.2g\"\n",
    "    norm = Normalize(vmin=vmin, vmax=vmax)\n",
    "    proxy_mappable = ScalarMappable(norm=norm, cmap=cmap)\n",
    "    ticks = np.linspace(vmin, vmax, nb_ticks)\n",
    "\n",
    "    ax.set_axis_off()\n",
    "\n",
    "    cax, kw = make_axes(ax, fraction=0.5, shrink=0.5)\n",
    "\n",
    "\n",
    "    fig.colorbar(\n",
    "        proxy_mappable,\n",
    "        cax=cax,\n",
    "        ticks=ticks,\n",
    "        orientation=\"vertical\",\n",
    "        format=cbar_tick_format,\n",
    "        ticklocation=\"left\",\n",
    "    )\n",
    "\n",
    "\n",
    "def make_contrast_plot(plot_data, target_map):\n",
    "\n",
    "\n",
    "    # targets = ['EEA', 'tf', 'SSRT', 'B']\n",
    "    targets = list(plot_data)\n",
    "\n",
    "    conditions = {\n",
    "        '': '',\n",
    "    'correctstop_correctgo': \"Correct Stop vs. Correct Go\",\n",
    "    'correctstop_incorrectgo': \"Correct Stop vs. Incorrect Go\",\n",
    "    'incorrectstop_correctstop': \"Incorrect Stop vs. Correct Stop\",\n",
    "    'incorrectstop_incorrectgo': \"Incorrect Stop vs. Incorrect Go\"}\n",
    "\n",
    "    targets = [''] + targets\n",
    "\n",
    "    fig, axs = plt.subplots(ncols=len(conditions), nrows=len(targets), figsize=(25,15))\n",
    "    fontsize = 15\n",
    "\n",
    "    max = get_global_contrast_minmax(plot_data)\n",
    "    min = -max\n",
    "\n",
    "\n",
    "    # axs[0, 0].set_axis_off()\n",
    "\n",
    "    make_colorbar(fig, axs[0,0], min, max, cmap='seismic')\n",
    "\n",
    "    for i, target in enumerate(targets):\n",
    "        \n",
    "        for j, cond in enumerate(conditions.keys()):\n",
    "\n",
    "            # column labels\n",
    "            if i == 0:\n",
    "                if cond == '':\n",
    "                    pass\n",
    "                else:\n",
    "                    ax = axs[0, j]\n",
    "                    ax.set_axis_off()\n",
    "                    # text = cond[0].title() + '\\n\\n' +  cond[1].replace('_', ' ').title()\n",
    "                    ax.text(0, 0.5, conditions[cond], fontsize=fontsize)\n",
    "            if j == 0:\n",
    "                # row labels\n",
    "                if target == '':\n",
    "                    pass\n",
    "                else:\n",
    "                    ax = axs[i, j]\n",
    "                    ax.set_axis_off()\n",
    "                    ax.text(0, 0.5, target_map[target], fontsize=fontsize)\n",
    "            if (i > 0) and (j > 0):\n",
    "\n",
    "                ax = axs[i, j]\n",
    "\n",
    "\n",
    "                data = plot_data[target]\n",
    "                lh = data['lh'][cond]\n",
    "                rh = data['rh'][cond]\n",
    "\n",
    "                ntp.plot(\n",
    "                    {'lh': lh, 'rh': rh},\n",
    "                    threshold=0,\n",
    "                    cmap='seismic',\n",
    "                    colorbar=False,\n",
    "                    vmin=min,\n",
    "                    vmax=max,\n",
    "                    ax=ax\n",
    "                )\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_contrast_plot(contrasts_plot, params['target_map'])\n",
    "plt.savefig(params['plot_output_path'] + 'ridge_contrast_fis_plot.png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "fis, best_fis, avg_fis, haufe_avg = pd.read_pickle(\n",
    "    params[\"model_results_path\"] + f\"contrasts_ridge_feature_importance.pkl\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_for_plotting(\n",
    "    fis: pd.DataFrame, correct: str, condition: str\n",
    ") -> pd.DataFrame:\n",
    "    tmp = fis[(fis[\"correct\"] == correct) & (fis[\"condition\"] == condition)]\n",
    "\n",
    "    tmp = tmp.drop(columns=[\"correct\", \"condition\"])\n",
    "    tmp[np.isnan(tmp)] = 0\n",
    "    return tmp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "lh, rh = broadcast_to_fsaverage(haufe_avg['EEA'])\n",
    "# haufe_avg['EEA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# format_for_plotting(lh, correct = 'correctstop', condition='correctgo')\n",
    "lh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "correct = [\"correct\", \"incorrect\"]\n",
    "cond = [\"go\", \"stop\"]\n",
    "\n",
    "correct_cond = list(product(correct, cond))\n",
    "correct_cond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "def strip_suffix(s: str):\n",
    "    \"\"\"Strip the first two parts of a string separated by underscores.\"\"\"\n",
    "    return \"_\".join(s.split(\"_\")[2:])\n",
    "\n",
    "def make_contrast(fpath: str, cond1: tuple, cond2: tuple) -> pd.DataFrame:\n",
    "    \"\"\"Make a contrast between two conditions.\n",
    "    Args:\n",
    "        fpath (str): Path to the folder containing the betas.\n",
    "        cond1 (tuple): First condition (string, string).\n",
    "        cond2 (tuple): Second condition (string, string).\n",
    "    Returns:\n",
    "        pd.DataFrame: Contrast between the two conditions.\n",
    "    \"\"\"\n",
    "\n",
    "    files = glob.glob(fpath + \"*.parquet\")\n",
    "    df1 = pd.read_parquet([f for f in files if cond1[0] in f][0])\n",
    "    df2 = pd.read_parquet([f for f in files if cond2[0] in f][0])\n",
    "\n",
    "    df1 = df1.fillna(0)\n",
    "    df2 = df2.fillna(0)\n",
    "\n",
    "    df1.columns = [strip_suffix(col) for col in df1.columns]\n",
    "    df2.columns = [strip_suffix(col) for col in df2.columns]\n",
    "\n",
    "    contrast_name = f\"{cond1[1]}_{cond2[1]}\"\n",
    "    df = df1 - df2\n",
    "    df = df.rename(columns={col: f\"{contrast_name}_{col}\" for col in df.columns})\n",
    "\n",
    "    return df\n",
    "def load_contrasts(params: dict) -> pd.DataFrame:\n",
    "    \"\"\"Load specific contrasts from processed betas.\"\"\"\n",
    "    return pd.concat(\n",
    "        [\n",
    "            make_contrast(\n",
    "                params[\"sst_betas_path\"], (\"cs\", \"correctstop\"), (\"cg\", \"correctgo\")\n",
    "            ),\n",
    "            make_contrast(\n",
    "                params[\"sst_betas_path\"], (\"cs\", \"correctstop\"), (\"ig\", \"incorrectgo\")\n",
    "            ),\n",
    "            make_contrast(\n",
    "                params[\"sst_betas_path\"], (\"is\", \"incorrectstop\"), (\"cs\", \"correctstop\")\n",
    "            ),\n",
    "            make_contrast(\n",
    "                params[\"sst_betas_path\"], (\"is\", \"incorrectstop\"), (\"ig\", \"incorrectgo\")\n",
    "            ),\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    "sst_contrasts = load_contrasts(params)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abcd_tools.utils.io import load_tabular\n",
    "\n",
    "mri_confounds_sst = load_tabular(params[\"mri_confounds_sst_path\"])\n",
    "sst_scopes = pd.read_pickle(params['sst_scopes_path'])\n",
    "sst_contrast_scopes = pd.read_pickle(params['sst_contrast_scopes_path'])\n",
    "targets_no_tf = load_tabular(params[\"targets_no_tf_path\"])\n",
    "targets_nback = load_tabular(params[\"nback_targets_path\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import BPt as bp\n",
    "\n",
    "def make_bpt_dataset(\n",
    "    betas: pd.DataFrame,\n",
    "    scopes: dict,\n",
    "    mri_confounds: pd.DataFrame,\n",
    "    targets: pd.DataFrame,\n",
    "    test_split=0.2,\n",
    "    random_state=123,\n",
    "    fpath: str = None,\n",
    ") -> bp.Dataset:\n",
    "    \"\"\"Create a BPt dataset from betas, confounds, and targets.\n",
    "\n",
    "    Args:\n",
    "        betas (pd.DataFrame): Concatenated betas.\n",
    "        scopes (dict): Scopes.\n",
    "        mri_confounds (pd.DataFrame): MRI confounds.\n",
    "        targets (pd.DataFrame): Behavioral targets.\n",
    "        test_split (float, optional): Test split. Defaults to 0.2.\n",
    "        random_state (int, optional): Random state. Defaults to 123.\n",
    "        fpath (str, optional): Path to save dataset. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        bp.Dataset: BPt dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.concat([betas, mri_confounds, targets], axis=1)\n",
    "    df = df.dropna(axis=1, how=\"all\").dropna(axis=1, how=\"all\").dropna()\n",
    "\n",
    "    dataset = bp.Dataset(df, targets=targets.columns.tolist())\n",
    "\n",
    "    scopes[\"covariates\"] = mri_confounds.columns.tolist()\n",
    "    for k, v in scopes.items():\n",
    "        dataset.add_scope(v, k, inplace=True)\n",
    "\n",
    "    dataset = dataset.auto_detect_categorical()\n",
    "    dataset = dataset.add_scope(\"mri_info_deviceserialnumber\", \"category\")\n",
    "    dataset = dataset.ordinalize(\"category\")\n",
    "\n",
    "    # deal with possible inf values\n",
    "    dataset = dataset.replace([np.inf, -np.inf], np.nan)\n",
    "    dataset = dataset.dropna()\n",
    "\n",
    "    dataset = dataset.set_test_split(test_split, random_state=random_state)\n",
    "\n",
    "    if fpath is not None:\n",
    "        dataset.to_pickle(fpath)\n",
    "        print(f\"Dataset saved to {fpath}\")\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "make_bpt_dataset(\n",
    "    sst_contrasts,\n",
    "    sst_contrast_scopes,\n",
    "    mri_confounds_sst,\n",
    "    targets_nback,\n",
    "    fpath=params[\"sst_nback_dataset_path\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
