{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import neurotools.plotting as ntp\n",
    "import seaborn as sns\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "from itertools import repeat\n",
    "\n",
    "from matplotlib.colors import Normalize\n",
    "from matplotlib.cm import ScalarMappable\n",
    "from matplotlib.colorbar import make_axes\n",
    "\n",
    "from nilearn.datasets import fetch_atlas_surf_destrieux\n",
    "\n",
    "from neurotools.plotting.ref import SurfRef\n",
    "\n",
    "from abcd_tools.image.preprocess import map_hemisphere\n",
    "\n",
    "from scipy.stats import false_discovery_control, pearsonr\n",
    "from itertools import product\n",
    "from abcd_tools.utils.ConfigLoader import load_yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cond = [\n",
    "    (\"correctstop\", \"correctgo\"),\n",
    "    (\"correctstop\", \"incorrectgo\"),\n",
    "    (\"incorrectstop\", \"correctstop\"),\n",
    "(\"incorrectstop\", \"incorrectgo\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = load_yaml(\"../parameters.yaml\")\n",
    "model = \"contrasts_ridge\"\n",
    "fis, best_fis, avg_fis, haufe_avg = pd.read_pickle(\n",
    "        params[\"model_results_path\"] + f\"{model}_feature_importance.pkl\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.datasets import fetch_atlas_surf_destrieux\n",
    "\n",
    "def map_destrieux(\n",
    "    lh: pd.DataFrame,\n",
    "    rh: pd.DataFrame,\n",
    "    prefix: str = \"\",\n",
    "    mask_non_significant=False,\n",
    "    use_fdr=False,\n",
    "    alpha=0.01,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Map Destrieux atlas.\n",
    "\n",
    "    Args:\n",
    "        lh (pd.DataFrame): Left hemisphere.\n",
    "        rh (pd.DataFrame): Right hemisphere.\n",
    "        prefix (str, optional): Prefix. Defaults to ''.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Mapped dataframe.\n",
    "    \"\"\"\n",
    "\n",
    "    dest = load_destrieux_atlas()\n",
    "\n",
    "    correct_values = pd.unique(lh[\"correct\"])\n",
    "    condition_values = pd.unique(lh[\"condition\"])\n",
    "\n",
    "    idx = [\"correct\", \"condition\"]\n",
    "\n",
    "    lh_df = pd.DataFrame()\n",
    "    rh_df = pd.DataFrame()\n",
    "\n",
    "    lh_tvalues = pd.DataFrame()\n",
    "    rh_tvalues = pd.DataFrame()\n",
    "\n",
    "    lh_pvalues = pd.DataFrame()\n",
    "    rh_pvalues = pd.DataFrame()\n",
    "\n",
    "    def _assemble_df(lh_mapped, rh_mapped, lh_correct, rh_correct, lh, rh):\n",
    "        lh_mapped.index = lh_correct.index\n",
    "        rh_mapped.index = rh_correct.index\n",
    "\n",
    "        lh_tmp = pd.concat([lh, lh_mapped])\n",
    "        rh_tmp = pd.concat([rh, rh_mapped])\n",
    "\n",
    "        return lh_tmp, rh_tmp\n",
    "\n",
    "    def apply_fdr(pvalues):\n",
    "        return pd.DataFrame(\n",
    "            false_discovery_control(pvalues, method=\"by\"),\n",
    "            index=pvalues.index,\n",
    "            columns=pvalues.columns,\n",
    "        )\n",
    "\n",
    "    for correct in correct_values:\n",
    "\n",
    "        for condition in condition_values:\n",
    "\n",
    "            lh_correct = lh[(lh[\"correct\"] == correct) & (lh[\"condition\"] == condition)]\n",
    "            rh_correct = rh[(rh[\"correct\"] == correct) & (rh[\"condition\"] == condition)]\n",
    "\n",
    "            lh_correct = lh_correct.set_index(idx)\n",
    "            rh_correct = rh_correct.set_index(idx)\n",
    "\n",
    "            lh_mapped, lh_t, lh_p = map_hemisphere(\n",
    "                lh_correct,\n",
    "                mapping=dest[\"map_left\"],\n",
    "                labels=dest[\"labels\"],\n",
    "                prefix=prefix,\n",
    "                suffix=\".lh\",\n",
    "                return_statistics=True,\n",
    "                decode_ascii=False,\n",
    "            )\n",
    "            rh_mapped, rh_t, rh_p = map_hemisphere(\n",
    "                rh_correct,\n",
    "                mapping=dest[\"map_right\"],\n",
    "                labels=dest[\"labels\"],\n",
    "                prefix=prefix,\n",
    "                suffix=\".rh\",\n",
    "                return_statistics=True,\n",
    "                decode_ascii=False,\n",
    "            )\n",
    "\n",
    "            lh_df, rh_df = _assemble_df(\n",
    "                lh_mapped, rh_mapped, lh_correct, rh_correct, lh_df, rh_df\n",
    "            )\n",
    "            lh_tvalues, rh_tvalues = _assemble_df(\n",
    "                lh_t, rh_t, lh_correct, rh_correct, lh_tvalues, rh_tvalues\n",
    "            )\n",
    "            lh_pvalues, rh_pvalues = _assemble_df(\n",
    "                lh_p, rh_p, lh_correct, rh_correct, lh_pvalues, rh_pvalues\n",
    "            )\n",
    "\n",
    "    if use_fdr:\n",
    "        lh_pvalues = apply_fdr(lh_pvalues)\n",
    "        rh_pvalues = apply_fdr(rh_pvalues)\n",
    "\n",
    "    if mask_non_significant:\n",
    "        lh_tvalues = lh_tvalues.mask(lh_pvalues > alpha)\n",
    "        rh_tvalues = rh_tvalues.mask(rh_pvalues > alpha)\n",
    "\n",
    "    df = pd.concat([lh_df, rh_df], axis=1)\n",
    "    vmin, vmax = get_fullrang_minmax(df)\n",
    "\n",
    "    return lh_df.reset_index(), rh_df.reset_index(), vmin, vmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lh, rh = broadcast_to_fsaverage(haufe_avg['EEA'])\n",
    "\n",
    "from scipy.stats import ttest_1samp\n",
    "\n",
    "def compute_tstat(mapping: dict) -> dict:\n",
    "     \"\"\"Compute t-statistics.\n",
    "\n",
    "     Args:\n",
    "         lh_mapping (pd.DataFrame): Left hemisphere mapping.\n",
    "\n",
    "     Returns:\n",
    "         dict: T-statistics.\n",
    "     \"\"\"\n",
    "\n",
    "     t_values = {}\n",
    "     p_values = {}\n",
    "\n",
    "     for roi, vertex in mapping.items():\n",
    "         t, p = ttest_1samp(vertex, 0, axis=0, nan_policy='omit')\n",
    "         t_values[roi] = t\n",
    "         p_values[roi] = p\n",
    "\n",
    "     return t_values, p_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_hemisphere(vertices: pd.DataFrame, mapping: np.array, labels: list,\n",
    "                   prefix: str=None, suffix: str=None,\n",
    "                   decode_ascii: bool=True, return_statistics: bool=False\n",
    "                   ) -> pd.DataFrame:\n",
    "    \"\"\"Map tabular vertexwise fMRI values to ROIs using nonzero average aggregation.\n",
    "\n",
    "    Args:\n",
    "        vertices (pd.DataFrame): Tabular vertexwise data (columns are vertices).\n",
    "        mapping (np.array): Array of ROI indices. Must be the same length as `vertices`.\n",
    "        labels (list): ROI labels for resulting averaged values.\n",
    "        prefix (str, optional): Prefix added to all column names. Defaults to None.\n",
    "        suffix (str, optional): Suffix added to all column names. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Nonzero-averaged ROIs.\n",
    "    \"\"\"\n",
    "\n",
    "    if decode_ascii:\n",
    "        labels = [label.decode() for label in labels]\n",
    "\n",
    "    map_dict = {}\n",
    "    avg_dict = {}\n",
    "\n",
    "    if isinstance(vertices, pd.DataFrame):\n",
    "        vertices = vertices.values\n",
    "\n",
    "    for idx in mapping:\n",
    "\n",
    "        indices = np.where(mapping == idx)[0]\n",
    "\n",
    "        map_dict[idx] = vertices[:, indices]\n",
    "        map_dict[idx][map_dict[idx] == 0] = np.nan\n",
    "        avg_dict[idx] = np.nanmean(map_dict[idx], axis=1)\n",
    "        map_dict[idx] = map_dict[idx]\n",
    "\n",
    "\n",
    "    def _assemble_df(collection: dict, labels, prefix, suffix) -> pd.DataFrame:\n",
    "        df = pd.DataFrame(collection, index=[0])\n",
    "        df = df.reindex(sorted(df.columns), axis=1)\n",
    "\n",
    "        if len(labels) > df.shape[1]:\n",
    "            labels = labels[1:]\n",
    "\n",
    "        labels = [prefix + str(label) + suffix for label in labels]\n",
    "        df.columns = labels\n",
    "\n",
    "        return df\n",
    "\n",
    "    rois = _assemble_df(avg_dict, labels, prefix, suffix)\n",
    "\n",
    "    if return_statistics:\n",
    "\n",
    "        tvalues, pvalues = compute_tstat(map_dict)\n",
    "\n",
    "        tvalues = _assemble_df(tvalues, labels, prefix, suffix)\n",
    "        pvalues = _assemble_df(pvalues, labels, prefix, suffix)\n",
    "\n",
    "        return rois, tvalues, pvalues\n",
    "    else:\n",
    "        return rois\n",
    "\n",
    "map_destrieux(lh, rh)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
