{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from abcd_tools.utils.ConfigLoader import load_yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = load_yaml(\"../parameters.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_vol_info(vol_info: pd.DataFrame) -> pd.DataFrame:\n",
    "\n",
    "    TPT_MAP = {\n",
    "        'baseline': 'baseline_year_1_arm_1',\n",
    "        '2year': '2_year_follow_up_y_arm_1',\n",
    "        '4year': '4_year_follow_up_y_arm_1',\n",
    "        '6year': '6_year_follow_up_y_arm_1',\n",
    "    }\n",
    "\n",
    "    tmp = vol_info.iloc[:, 0].str.split(\"_\", expand=True)[[2, 3]]\n",
    "    tmp.columns = ['src_subject_id', 'eventname']\n",
    "    tmp['src_subject_id'] = 'NDAR_' + tmp['src_subject_id']\n",
    "    tmp['eventname'] = tmp['eventname'].map(TPT_MAP)\n",
    "\n",
    "    return tmp\n",
    "\n",
    "def concatenate_hemispheres(lh: pd.DataFrame, rh: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Concatenate left and right hemisphere dataframes\n",
    "\n",
    "    Args:\n",
    "        lh (pd.DataFrame): Left hemisphere data\n",
    "        rh (pd.DataFrame): Right hemisphere data\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Concatenated data\n",
    "    \"\"\"\n",
    "    lh.columns = [c + '_lh' for c in lh.columns]\n",
    "    rh.columns = [c + '_rh' for c in rh.columns]\n",
    "    return pd.concat([lh, rh], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_betas(sst_conditions: dict, hemispheres: list, beta_input_dir: str,\n",
    "    vol_info: pd.DataFrame,release: str='r5') -> None:\n",
    "    \n",
    "    betas_out = pd.DataFrame()\n",
    "    idx = ['src_subject_id', 'eventname', 'run']\n",
    "    for condition in sst_conditions.keys():\n",
    "        betas = {}\n",
    "        for hemi in hemispheres:\n",
    "\n",
    "            if release == 'r5':\n",
    "                run1_fpath = f\"{beta_input_dir}SST_1_{sst_conditions[condition]}-{hemi}.parquet\"\n",
    "                run2_fpath = f\"{beta_input_dir}SST_2_{sst_conditions[condition]}-{hemi}.parquet\"\n",
    "            elif release == 'r6':\n",
    "                run1_fpath = f\"{beta_input_dir}sst_{condition}_beta_r01_{hemi}.parquet\"\n",
    "                run2_fpath = f\"{beta_input_dir}sst_{condition}_beta_r02_{hemi}.parquet\"\n",
    "\n",
    "            run1 = pd.read_parquet(run1_fpath)\n",
    "            run2 = pd.read_parquet(run2_fpath)\n",
    "\n",
    "            run1['run'] = 1\n",
    "            run2['run'] = 2\n",
    "\n",
    "            run1 = pd.concat([vol_info, run1], axis=1)\n",
    "            run2 = pd.concat([vol_info, run2], axis=1)\n",
    "\n",
    "            combined = pd.concat([run1, run2])\n",
    "            name = sst_conditions[condition]\n",
    "\n",
    "            betas[hemi] = combined\n",
    "\n",
    "        betas_df = concatenate_hemispheres(betas['lh'].set_index(idx), betas['rh'].set_index(idx))\n",
    "        name = sst_conditions[condition]\n",
    "        betas_df.columns = [c.replace('tableData', name + '_') for c in betas_df.columns]\n",
    "\n",
    "        betas_df['condition'] = condition\n",
    "        betas_out = pd.concat([betas_out, betas_df])\n",
    "\n",
    "    return betas_out.reset_index()\n",
    "\n",
    "        # betas_df.to_parquet(f\"{beta_output_dir}average_betas_{condition}.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load unprocessed betas for one condition (correct go)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vol_info_r5 = pd.read_parquet(params['vol_info_path_r5'])\n",
    "vol_info_r6 = pd.read_parquet(params['vol_info_path_r6'])\n",
    "vol_info_r6 = parse_vol_info(vol_info_r6)\n",
    "\n",
    "sst_conditions = {'cg': 'correct_go'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# r5 = combine_betas(sst_conditions, \n",
    "#                     params['hemispheres'], \n",
    "#                     params['beta_input_dir_r5'], \n",
    "#                     vol_info_r5, \n",
    "#                     release='r5')\n",
    "# r5 = r5[r5['eventname'] == 'baseline_year_1_arm_1']\n",
    "# r5 = r5.dropna()\n",
    "# r5.to_parquet(\"../../data/02_intermediate/cg_r5_combined_betas.parquet\")\n",
    "r5 = pd.read_parquet(\"../../data/02_intermediate/cg_r5_combined_betas.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# r6 = combine_betas(sst_conditions,\n",
    "#                     params['hemispheres'],\n",
    "#                     params['beta_input_dir_r6'],\n",
    "#                     vol_info_r6,\n",
    "#                     release='r6')\n",
    "# r6 = r6[r6['eventname'] == 'baseline_year_1_arm_1']\n",
    "# r6 = r6.dropna()\n",
    "# r6.to_parquet(\"../../data/02_intermediate/cg_r6_combined_betas.parquet\")\n",
    "r6 = pd.read_parquet(\"../../data/02_intermediate/cg_r6_combined_betas.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick a random subset of subjects\n",
    "np.random.seed(64)\n",
    "subjects = r5['src_subject_id'].unique()\n",
    "selected_subjects = np.random.choice(subjects, 15, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_betas(r5, r6, subjects):\n",
    "    r5 = r5[r5['src_subject_id'].isin(subjects)]\n",
    "    r6 = r6[r6['src_subject_id'].isin(subjects)]\n",
    "\n",
    "    id_vars = ['src_subject_id', 'eventname', 'run', 'condition']\n",
    "    idx = id_vars + ['variable']\n",
    "    r5 = r5.melt(id_vars=id_vars).set_index(idx)\n",
    "    r6 = r6.melt(id_vars=id_vars).set_index(idx)\n",
    "\n",
    "    r5.rename(columns={'value': 'r5'}, inplace=True)\n",
    "    r6.rename(columns={'value': 'r6'}, inplace=True)\n",
    "    \n",
    "    return pd.concat([r5, r6], axis=1)\n",
    "\n",
    "combined = combine_betas(r5, r6, selected_subjects).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_betas(subset, xvar='r5_cg', yvar='r6_cg', hue=None):\n",
    "\n",
    "    g = sns.FacetGrid(subset, col='src_subject_id', col_wrap=5, height=3, hue=hue)\n",
    "    g.map(sns.scatterplot, xvar,  yvar)\n",
    "\n",
    "    if hue is not None:\n",
    "        g.add_legend()\n",
    "\n",
    "    for ax in g.axes.flat:\n",
    "        ax.axline((0, 0), slope=1, color='k', ls='--')\n",
    "        ax.grid(True, axis='both', linestyle=':')\n",
    "        ax.set_xlim(-10, 10) \n",
    "        ax.set_ylim(-10, 10)\n",
    "        ax.set_aspect('equal', adjustable='box')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_betas(combined.reset_index(), xvar='r5', yvar='r6', hue='run')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine Average Betas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_average_betas(params, subjects, fpath=\"../../data/02_intermediate/average_betas.parquet\"):\n",
    "\n",
    "    r5_cg_avg = pd.read_parquet(params['beta_output_dir_r5'] + 'average_betas_cg.parquet').reset_index()\n",
    "    r6_cg_avg = pd.read_parquet(params['beta_output_dir_r6'] + 'average_betas_cg.parquet').reset_index()\n",
    "\n",
    "    r5_cg_avg = r5_cg_avg[r5_cg_avg['src_subject_id'].isin(subjects)]\n",
    "    r6_cg_avg = r6_cg_avg[r6_cg_avg['src_subject_id'].isin(subjects)]\n",
    "\n",
    "    r5_long = r5_cg_avg.melt(id_vars=['src_subject_id', 'eventname'], var_name='vertex', value_name='r5_cg')\n",
    "    r6_long = r6_cg_avg.melt(id_vars=['src_subject_id', 'eventname'], var_name='vertex', value_name='r6_cg')\n",
    "\n",
    "    idx = ['src_subject_id', 'eventname', 'vertex']\n",
    "    long_compare = pd.concat([r5_long.set_index(idx), r6_long.set_index(idx)], axis=1)\n",
    "    long_compare = long_compare.reset_index()\n",
    "\n",
    "    # persist; this is a big df\n",
    "    long_compare.to_parquet(fpath)\n",
    "\n",
    "    return long_compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpath = \"../../data/02_intermediate/cg_average_betas.parquet\"\n",
    "avg_betas = combine_average_betas(params, selected_subjects, fpath=fpath)\n",
    "# avg_betas = pd.read_parquet(fpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_betas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_betas(avg_betas)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
