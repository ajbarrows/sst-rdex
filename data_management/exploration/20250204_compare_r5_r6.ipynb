{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "# import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "\n",
    "from typing import Dict, List\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from abcd_tools.utils.io import load_tabular\n",
    "from abcd_tools.utils.ConfigLoader import load_yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = load_yaml(\"../parameters.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_vol_info(vol_info: pl.DataFrame) -> pl.DataFrame:\n",
    "    \"\"\"Parse volume information to extract subject ID and event name\n",
    "    \n",
    "    Args:\n",
    "        vol_info (pl.DataFrame): Volume information DataFrame\n",
    "        \n",
    "    Returns:\n",
    "        pl.DataFrame: Parsed information with subject ID and event name\n",
    "    \"\"\"\n",
    "    TPT_MAP = {\n",
    "        'baseline': 'baseline_year_1_arm_1',\n",
    "        '2year': '2_year_follow_up_y_arm_1',\n",
    "        '4year': '4_year_follow_up_y_arm_1',\n",
    "        '6year': '6_year_follow_up_y_arm_1',\n",
    "    }\n",
    "    \n",
    "    return (\n",
    "        vol_info\n",
    "        .select(pl.col(vol_info.columns[0]))  # Select first column\n",
    "        .with_columns([\n",
    "            pl.col(vol_info.columns[0])\n",
    "            .str.split('_')\n",
    "            .list.get(2)\n",
    "            .alias('src_subject_id'),\n",
    "            \n",
    "            pl.col(vol_info.columns[0])\n",
    "            .str.split('_')\n",
    "            .list.get(3)\n",
    "            .alias('eventname')\n",
    "        ])\n",
    "        .with_columns([\n",
    "            pl.col('src_subject_id').map_elements(lambda x: f'NDAR_{x}'),\n",
    "            pl.col('eventname').replace_strict(TPT_MAP)\n",
    "        ])\n",
    "        .select(['src_subject_id', 'eventname'])\n",
    "    )\n",
    "\n",
    "def concatenate_hemispheres(lh: pl.DataFrame, rh: pl.DataFrame) -> pl.DataFrame:\n",
    "    \"\"\"Concatenate left and right hemisphere dataframes\n",
    "    \n",
    "    Args:\n",
    "        lh (pl.DataFrame): Left hemisphere data\n",
    "        rh (pl.DataFrame): Right hemisphere data\n",
    "        \n",
    "    Returns:\n",
    "        pl.DataFrame: Concatenated data\n",
    "    \"\"\"\n",
    "    idx = ['src_subject_id', 'eventname', 'run']\n",
    "    lh = lh.select([\n",
    "        pl.col('*').name.map(lambda x: x + '_lh' if x not in idx else x)\n",
    "    ])\n",
    "    rh = rh.select([\n",
    "        pl.col('*').name.map(lambda x: x + '_rh' if x not in idx else x)\n",
    "    ])\n",
    "    return pl.concat([lh, rh], how='align')\n",
    "\n",
    "def load_degrees_of_freedom(r1_fpath: str, r2_fpath: str) -> pd.DataFrame:\n",
    "    \"\"\"Load censored frame information for run averaging.\n",
    "\n",
    "    Args:\n",
    "        r1_fpath (str): Filepath to run 1 info\n",
    "        r2_fpath (str): Filepath to run 2 info\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DOFs for runs 1 and 2\n",
    "    \"\"\"\n",
    "    r1_dof = load_tabular(r1_fpath, cols=['tfmri_sstr1_beta_dof'])\n",
    "    r2_dof = load_tabular(r2_fpath, cols=['tfmri_sstr2_beta_dof'])\n",
    "\n",
    "    return pd.concat([r1_dof, r2_dof], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_betas(\n",
    "    sst_conditions: Dict[str, str],\n",
    "    hemispheres: List[str],\n",
    "    beta_input_dir: str,\n",
    "    vol_info: pl.DataFrame,\n",
    "    subjects: list,\n",
    "    release: str = 'r5'\n",
    ") -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Combine beta values from different conditions and hemispheres into a single DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        sst_conditions: Dictionary mapping condition names to their identifiers\n",
    "        hemispheres: List of hemispheres to process (e.g., ['lh', 'rh'])\n",
    "        beta_input_dir: Directory containing the beta parquet files\n",
    "        vol_info: DataFrame containing volume information\n",
    "        release: Release version ('r5' or 'r6')\n",
    "        \n",
    "    Returns:\n",
    "        Combined DataFrame with beta values for all conditions and hemispheres\n",
    "    \"\"\"\n",
    "    betas_out = pl.DataFrame()\n",
    "    idx = ['src_subject_id', 'eventname', 'run']\n",
    "    \n",
    "    for condition in sst_conditions.keys():\n",
    "        betas = {}\n",
    "        \n",
    "        for hemi in hemispheres:\n",
    "\n",
    "            # Construct file paths based on release version\n",
    "            if release == 'r6':\n",
    "                run1_fpath = f\"{beta_input_dir}sst_{condition}_beta_r01_{hemi}.parquet\"\n",
    "                run2_fpath = f\"{beta_input_dir}sst_{condition}_beta_r02_{hemi}.parquet\"\n",
    "            else:\n",
    "                run1_fpath = f\"{beta_input_dir}SST_1_{sst_conditions[condition]}-{hemi}.parquet\"\n",
    "                run2_fpath = f\"{beta_input_dir}SST_2_{sst_conditions[condition]}-{hemi}.parquet\"\n",
    "            \n",
    "            # Read parquet files\n",
    "            run1 = pl.read_parquet(run1_fpath)\n",
    "            run2 = pl.read_parquet(run2_fpath)\n",
    "            \n",
    "            # Add run column\n",
    "            run1 = run1.with_columns(pl.lit(1).alias('run'))\n",
    "            run2 = run2.with_columns(pl.lit(2).alias('run'))\n",
    "            \n",
    "            # Combine with vol_info\n",
    "            run1 = vol_info.hstack(run1)\n",
    "            run2 = vol_info.hstack(run2)\n",
    "\n",
    "            # Combine runs\n",
    "            combined = pl.concat([run1, run2])\n",
    "            betas[hemi] = combined\n",
    "        \n",
    "        # Concatenate hemispheres\n",
    "        betas_df = concatenate_hemispheres(betas['lh'], betas['rh'])\n",
    "        \n",
    "        # Rename columns\n",
    "        betas_df = betas_df.rename(lambda x: x.replace('tableData', '')\n",
    "                        if 'tableData' in x else x)\n",
    "                        \n",
    "        # Add condition column\n",
    "        betas_df = betas_df.with_columns(pl.lit(condition).alias('condition'))\n",
    "        \n",
    "        # Filter for baseline\n",
    "        betas_df = betas_df.filter(pl.col('eventname') == 'baseline_year_1_arm_1')\n",
    "        betas_df = betas_df.filter(pl.col('src_subject_id').is_in(subjects))\n",
    "        betas_df = betas_df.drop_nulls()\n",
    "            \n",
    "        betas_df = betas_df.unpivot(\n",
    "            index=['src_subject_id', 'eventname', 'run', 'condition'],\n",
    "            variable_name='vertex', value_name=f'{release}_beta'\n",
    "        )\n",
    "        # Concatenate with previous results\n",
    "        betas_out = pl.concat([betas_out, betas_df]) if not betas_out.is_empty() else betas_df\n",
    "\n",
    "    # replace 0 with Null\n",
    "    betas_out = betas_out.with_columns(pl.col(f'{release}_beta').replace(0, None))\n",
    "    \n",
    "    return betas_out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load unprocessed betas for one condition (correct go)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vol_info_r4 = pl.read_parquet(params['vol_info_path_r4'])\n",
    "vol_info_r5 = pl.read_parquet(params['vol_info_path_r5'])\n",
    "vol_info_r6 = pl.read_parquet(params['vol_info_path_r6'])\n",
    "vol_info_r6 = parse_vol_info(vol_info_r6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select a few random subjects from release 4 (not all will carry through because of missingness)\n",
    "sample_subjects = vol_info_r4.filter(pl.col('eventname') == 'baseline_year_1_arm_1').sample(5, seed=42)\n",
    "sample_subjects = sample_subjects['src_subject_id'].to_list()\n",
    "sample_subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sst_conditions = {\n",
    "    'cs': 'correct_stop',\n",
    "    'cg': 'correct_go',\n",
    "    'is': 'incorrect_stop',\n",
    "    'ig': 'incorrect_go',\n",
    "    'csvcg': 'correct_stop_vs_correct_go',\n",
    "    'igvcg': 'incorrect_go_vs_correct_go'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r5 = combine_betas(sst_conditions, \n",
    "                    ['lh', 'rh'], \n",
    "                    params['beta_input_dir_r5'], \n",
    "                    vol_info_r5,\n",
    "                    sample_subjects, \n",
    "                    release='r5')\n",
    "\n",
    "r5.write_parquet(\"../../data/02_intermediate/r5_betas.parquet\")\n",
    "r5 = pl.read_parquet(\"../../data/02_intermediate/r5_betas.parquet\")\n",
    "r5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r6 = combine_betas(sst_conditions,\n",
    "                    ['lh', 'rh'],\n",
    "                    params['beta_input_dir_r6'],\n",
    "                    vol_info_r6,\n",
    "                    sample_subjects,\n",
    "                    release='r6')\n",
    "r6.write_parquet(\"../../data/02_intermediate/r6_betas.parquet\")\n",
    "r6 = pl.read_parquet(\"../../data/02_intermediate/r6_betas.parquet\")\n",
    "r6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined = pl.concat([r5, r6], how='align')\n",
    "joined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Releases 5 and 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_betas(subset, row = 'src_subject_id', col = 'condition', xvar='r5_cg', yvar='r6_cg', hue=None):\n",
    "\n",
    "    g = sns.FacetGrid(subset, col=col, row=row, hue=hue)\n",
    "    g.map(sns.scatterplot, xvar,  yvar)\n",
    "    g.set_titles(col_template=\"{col_name}\", row_template=\"{row_name}\")\n",
    "\n",
    "    if hue is not None:\n",
    "        g.add_legend()\n",
    "\n",
    "    for ax in g.axes.flat:\n",
    "        ax.axline((0, 0), slope=1, color='k', ls='--')\n",
    "        ax.grid(True, axis='both', linestyle=':')\n",
    "        ax.set_xlim(-10, 10) \n",
    "        ax.set_ylim(-10, 10)\n",
    "        ax.set_aspect('equal', adjustable='box')\n",
    "    \n",
    "\n",
    "plot_betas(joined.to_pandas(), xvar='r5_beta', yvar='r6_beta', hue='run')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Release 5 to Release 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I didn't grab these conditions for r4\n",
    "sst_conditions.pop('csvcg')\n",
    "sst_conditions.pop('igvcg')\n",
    "sst_conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r4 = combine_betas(sst_conditions,\n",
    "                    ['lh', 'rh'],\n",
    "                    params['beta_input_dir_r4'],\n",
    "                    vol_info_r4,\n",
    "                    sample_subjects,\n",
    "                    release='r4')\n",
    "r4.write_parquet(\"../../data/02_intermediate/r4_betas.parquet\")\n",
    "r4 = pl.read_parquet(\"../../data/02_intermediate/r4_betas.parquet\")\n",
    "r4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined = pl.concat([r4, r5], how='align')\n",
    "joined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_betas(joined.to_pandas().dropna(), xvar='r4_beta', yvar='r5_beta', hue='run')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine Average Betas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def combine_average_betas(params, subjects, fpath=\"../../data/02_intermediate/average_betas.parquet\"):\n",
    "\n",
    "#     r5_cg_avg = pd.read_parquet(params['beta_output_dir_r5'] + 'average_betas_cg.parquet').reset_index()\n",
    "#     r6_cg_avg = pd.read_parquet(params['beta_output_dir_r6'] + 'average_betas_cg.parquet').reset_index()\n",
    "\n",
    "#     r5_cg_avg = r5_cg_avg[r5_cg_avg['src_subject_id'].isin(subjects)]\n",
    "#     r6_cg_avg = r6_cg_avg[r6_cg_avg['src_subject_id'].isin(subjects)]\n",
    "\n",
    "#     r5_long = r5_cg_avg.melt(id_vars=['src_subject_id', 'eventname'], var_name='vertex', value_name='r5_cg')\n",
    "#     r6_long = r6_cg_avg.melt(id_vars=['src_subject_id', 'eventname'], var_name='vertex', value_name='r6_cg')\n",
    "\n",
    "#     idx = ['src_subject_id', 'eventname', 'vertex']\n",
    "#     long_compare = pd.concat([r5_long.set_index(idx), r6_long.set_index(idx)], axis=1)\n",
    "#     long_compare = long_compare.reset_index()\n",
    "\n",
    "#     # persist; this is a big df\n",
    "#     long_compare.to_parquet(fpath)\n",
    "\n",
    "#     return long_compare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine Degrees of Freedom (used in averaging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dof_r5 = load_degrees_of_freedom(params['mri_r1_dof_path_r5'], params['mri_r2_dof_path_r5'])\n",
    "# dof_r6 = load_degrees_of_freedom(params['mri_r1_dof_path_r6'], params['mri_r2_dof_path_r6'])\n",
    "\n",
    "# def reshape_dof(dof):\n",
    "#     dof.columns = ['run1', 'run2']\n",
    "#     dof = dof.reset_index().melt(id_vars=['src_subject_id', 'eventname'], var_name='run', value_name='dof')\n",
    "#     return dof.set_index(['src_subject_id', 'eventname', 'run'])\n",
    "\n",
    "# dof_r5 = reshape_dof(dof_r5)\n",
    "# dof_r6 = reshape_dof(dof_r6)\n",
    "\n",
    "# dof = dof_r5.join(dof_r6, lsuffix='_r5', rsuffix='_r6').reset_index()\n",
    "# dof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.scatterplot(data=dof.dropna(), x='dof_r5', y='dof_r6', hue='run', alpha=0.5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
