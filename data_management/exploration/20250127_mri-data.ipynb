{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from abcd_tools.utils.io import load_tabular\n",
    "from abcd_tools.utils.ConfigLoader import load_yaml\n",
    "from abcd_tools.image.preprocess import compute_average_betas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = load_yaml(\"../parameters.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_degrees_of_freedom(r1_fpath: str, r2_fpath: str) -> pd.DataFrame:\n",
    "    \"\"\"Load censored frame information for run averaging.\n",
    "\n",
    "    Args:\n",
    "        r1_fpath (str): Filepath to run 1 info\n",
    "        r2_fpath (str): Filepath to run 2 info\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DOFs for runs 1 and 2\n",
    "    \"\"\"\n",
    "    r1_dof = load_tabular(r1_fpath, cols=['tfmri_sstr1_beta_dof'])\n",
    "    r2_dof = load_tabular(r2_fpath, cols=['tfmri_sstr2_beta_dof'])\n",
    "\n",
    "    return pd.concat([r1_dof, r2_dof], axis=1)\n",
    "\n",
    "dof_r5 = load_degrees_of_freedom(params['mri_r1_dof_path_r5'], params['mri_r2_dof_path_r5'])\n",
    "dof_r6 = load_degrees_of_freedom(params['mri_r1_dof_path_r6'], params['mri_r2_dof_path_r6'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vol_info_path_r5 = params['vol_info_path_r5']\n",
    "vol_info_path_r6 = params['vol_info_path_r6']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_hemispheres(lh: pd.DataFrame, rh: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Concatenate left and right hemisphere dataframes\n",
    "\n",
    "    Args:\n",
    "        lh (pd.DataFrame): Left hemisphere data\n",
    "        rh (pd.DataFrame): Right hemisphere data\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Concatenated data\n",
    "    \"\"\"\n",
    "    lh.columns = [c + '_lh' for c in lh.columns]\n",
    "    rh.columns = [c + '_rh' for c in rh.columns]\n",
    "    return pd.concat([lh, rh], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_input_dir_r5 = params['beta_input_dir_r5']\n",
    "beta_input_dir_r6 = params['beta_input_dir_r6']\n",
    "\n",
    "beta_output_dir_r5 = params['beta_output_dir_r5']\n",
    "beta_output_dir_r6 = params['beta_output_dir_r6']\n",
    "\n",
    "processed_beta_dir_r5 = params['processed_beta_dir_r5']\n",
    "processed_beta_dir_r6 = params['processed_beta_dir_r6']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run1 = pd.read_parquet(beta_input_dir_r5 + 'SST_1_correct_go-lh.parquet')\n",
    "# run2 = pd.read_parquet(beta_input_dir_r5 + 'SST_2_correct_go-lh.parquet')\n",
    "# run1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run1 = pd.read_parquet(beta_input_dir_r6 + 'sst_cg_beta_r01_lh.parquet')\n",
    "run2 = pd.read_parquet(beta_input_dir_r6 + 'sst_cg_beta_r02_lh.parquet')\n",
    "run1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_vol_info(vol_info: pd.DataFrame) -> pd.DataFrame:\n",
    "\n",
    "    TPT_MAP = {\n",
    "        'baseline': 'baseline_year_1_arm_1',\n",
    "        '2year': '2_year_follow_up_y_arm_1',\n",
    "        '4year': '4_year_follow_up_y_arm_1',\n",
    "        '6year': '6_year_follow_up_y_arm_1',\n",
    "    }\n",
    "\n",
    "    tmp = vol_info.iloc[:, 0].str.split(\"_\", expand=True)[[2, 3]]\n",
    "    tmp.columns = ['src_subject_id', 'eventname']\n",
    "    tmp['src_subject_id'] = 'NDAR_' + tmp['src_subject_id']\n",
    "    tmp['eventname'] = tmp['eventname'].map(TPT_MAP)\n",
    "\n",
    "    return tmp\n",
    "vol_info_r6 = pd.read_parquet(vol_info_path_r6)\n",
    "vol_info_r6 = parse_vol_info(vol_info_r6)\n",
    "vol_info_r6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_average_betas(run1: pd.DataFrame, run2: pd.DataFrame, \n",
    "    vol_info: pd.DataFrame, motion: pd.DataFrame,\n",
    "    name: str, release='r6') -> pd.DataFrame:\n",
    "\n",
    "    run1 = pd.concat([run1, vol_info], axis=1)\n",
    "    run2 = pd.concat([run2, vol_info], axis=1)\n",
    "\n",
    "    if release == 'r5':\n",
    "        run1 = run1[run1['eventname'] == 'baseline_year_1_arm_1']\n",
    "        run2 = run2[run2['eventname'] == 'baseline_year_1_arm_1']\n",
    "\n",
    "        motion = motion.reset_index()\n",
    "        motion = motion[motion['eventname'] == 'baseline_year_1_arm_1']\n",
    "        motion = motion.set_index(['src_subject_id', 'eventname'])\n",
    "\n",
    "    def _align(run1, run2, motion):\n",
    "        \"\"\"Align dataframes on index and columns.\"\"\"\n",
    "        motion.columns = ['run1_dof', 'run2_dof']\n",
    "\n",
    "        run1, run2 = run1.align(run2, axis=1)\n",
    "        run1, motion = run1.align(motion, axis=0)\n",
    "        run2, motion = run2.align(motion, axis=0)\n",
    "\n",
    "        return run1, run2, motion\n",
    "    \n",
    "    idx = ['src_subject_id', 'eventname']\n",
    "    run1 = run1.set_index(idx)\n",
    "    run2 = run2.set_index(idx)\n",
    "\n",
    "    run1_stripped, run2_stripped, motion = _align(run1, run2, motion)\n",
    "    \n",
    "    # Betas == 0 are not included in the average\n",
    "    run1_stripped[run1_stripped == 0] = np.nan\n",
    "    run2_stripped[run2_stripped == 0] = np.nan\n",
    "\n",
    "    # multiply Beta values by degrees of freedom\n",
    "    run1_weighted = run1_stripped.mul(motion['run1_dof'], axis=0)\n",
    "    run2_weighted = run2_stripped.mul(motion['run2_dof'], axis=0)\n",
    "\n",
    "    # divide sum by total degrees of freedom\n",
    "    num = run1_weighted.add(run2_weighted, axis=0)\n",
    "    den = motion['run1_dof'] + motion['run2_dof']\n",
    "    avg = num.div(den, axis=0)\n",
    "\n",
    "    avg.columns = [c.replace('tableData', name + '_') for c in avg.columns]\n",
    "\n",
    "   # remove columns and rows that are all missing, then remove rows missing anything\n",
    "\n",
    "\n",
    "    return avg.dropna(how='all', axis=1).dropna(how='all', axis=0).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_average_betas(run1, run2, vol_info_r6, dof_r6, 'correct_go', release='r6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_input_dir = \"../../data/02_intermediate/\"\n",
    "beta_output_dir = \"../../data/02_intermediate/avg_betas/\"\n",
    "\n",
    "def combine_betas(sst_conditions: dict, hemispheres: list, dof: pd.DataFrame, beta_input_dir: str, beta_output_dir: str,\n",
    "    vol_info_path: str,release: str='r5') -> None:\n",
    "    \"\"\"Combine betas for SST conditions\n",
    "\n",
    "    Args:\n",
    "        sst_conditions (dict): SST conditions\n",
    "        hemispheres (list): Hemispheres\n",
    "        beta_input_dir (str): Directory containing beta data\n",
    "        beta_output_dir (str): Directory to save combined betas\n",
    "        vol_info_path (str): Path to volume information\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    vol_info = pd.read_parquet(vol_info_path)\n",
    "\n",
    "    for condition in sst_conditions.keys():\n",
    "        betas = {}\n",
    "        for hemi in hemispheres:\n",
    "\n",
    "            if release == 'r5':\n",
    "                run1_fpath = f\"{beta_input_dir}SST_1_{sst_conditions[condition]}-{hemi}.parquet\"\n",
    "                run2_fpath = f\"{beta_input_dir}SST_2_{sst_conditions[condition]}-{hemi}.parquet\"\n",
    "            elif release == 'r6':\n",
    "                run1_fpath = f\"{beta_input_dir}sst_{condition}_beta_r01_{hemi}.parquet\"\n",
    "                run2_fpath = f\"{beta_input_dir}sst_{condition}_beta_r02_{hemi}.parquet\"\n",
    "\n",
    "            run1 = pd.read_parquet(run1_fpath)\n",
    "            run2 = pd.read_parquet(run2_fpath)\n",
    "\n",
    "            name = sst_conditions[condition]\n",
    "            avg_betas = compute_average_betas(run1, run2, vol_info, dof, name=name, release=release)\n",
    "\n",
    "            betas[hemi] = avg_betas\n",
    "\n",
    "        betas_df = concatenate_hemispheres(betas['lh'], betas['rh'])\n",
    "\n",
    "        betas_df.to_parquet(f\"{beta_output_dir}average_betas_{condition}.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sst_conditions = {\n",
    "    'cs': 'correct_stop',\n",
    "    'cg': 'correct_go',\n",
    "    'is': 'incorrect_stop',\n",
    "    'ig': 'incorrect_go'\n",
    "}\n",
    "hemispheres = ['lh', 'rh']\n",
    "\n",
    "combine_betas(sst_conditions, hemispheres, dof_r5, beta_input_dir_r5, beta_output_dir_r5, vol_info_path_r5, release='r5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_parquet(beta_output_dir_r5 + 'average_betas_cs.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mri_qc(mri_qc_path: str) -> pd.DataFrame:\n",
    "    mri_qc = load_tabular(mri_qc_path, cols=['imgincl_sst_include'])\n",
    "    return mri_qc[mri_qc['imgincl_sst_include'] == 1]\n",
    "    \n",
    "mri_qc_path = \"../../data/01_raw/tabular/core/imaging/mri_y_qc_incl.csv\"\n",
    "mri_qc = load_mri_qc(mri_qc_path)\n",
    "mri_qc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_tabular(params['filtered_behavioral_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_avg_betas(mri_qc_df: pd.DataFrame, \n",
    "                    sst_conditions: list,\n",
    "                    filtered_behavioral_path: str,\n",
    "                    beta_output_dir: str,\n",
    "                    processed_beta_dir: str) -> pd.DataFrame:\n",
    "    \"\"\"Filter average betas based on QC data\n",
    "\n",
    "    Args:\n",
    "        mri_qc_df (pd.DataFrame): MRI QC data\n",
    "        sst_conditions (list): SST conditions\n",
    "        beta_output_dir (str): Path to average betas\n",
    "        processed_beta_dir (str): Path to processed betas\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    # load targets\n",
    "    filtered_behavioral = load_tabular(filtered_behavioral_path)\n",
    "    n_targets = filtered_behavioral.shape[0]\n",
    "    \n",
    "    for condition in sst_conditions:\n",
    "        avg_betas_fpath = f\"{beta_output_dir}average_betas_{condition}.parquet\"\n",
    "        avg_betas = pd.read_parquet(avg_betas_fpath)\n",
    "\n",
    "        # limit to available targets\n",
    "        avg_betas = avg_betas[avg_betas.index.isin(filtered_behavioral.index)]\n",
    "\n",
    "        print(f\"{avg_betas.shape[0]} of {n_targets} subjects had MRI data for {condition}\")\n",
    "\n",
    "        nrows_before = avg_betas.shape[0]\n",
    "        avg_betas = avg_betas[avg_betas.index.isin(mri_qc_df.index)]\n",
    "\n",
    "        diff = nrows_before - avg_betas.shape[0]\n",
    "\n",
    "        print(f\"{diff} failed MRI QC for {condition}\")\n",
    "\n",
    "        avg_betas.to_parquet(f\"{processed_beta_dir}processed_betas_{condition}.parquet\")\n",
    "\n",
    "        del(avg_betas)\n",
    "\n",
    "\n",
    "filter_avg_betas(mri_qc, sst_conditions.keys(),\n",
    "    params['filtered_behavioral_path'], beta_output_dir_r5, processed_beta_dir_r5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_parquet(beta_output_dir_r5 + 'average_betas_cs.parquet')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_parquet(f\"{processed_beta_dir_r5}processed_betas_cs.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "motion_path = \"../../data/01_raw/abcd-sync/6.0/tabulated/img/mriqcrp203.csv\"\n",
    "scanner_path = \"../../data/01_raw/abcd-sync/6.0/tabulated/img/abcd_mri01.csv\"\n",
    "\n",
    "def load_mri_confounds(motion_path: str, scanner_path: str, timepoints: list) -> pd.DataFrame:\n",
    "    motion = load_tabular(motion_path, cols=['iqc_sst_all_mean_motion'], timepoints=timepoints)\n",
    "    scanner = load_tabular(scanner_path, cols = ['mri_info_deviceserialnumber'], timepoints=timepoints)\n",
    "\n",
    "    return pd.concat([motion, scanner], axis=1)\n",
    "    \n",
    "mri_confounds = pd.concat([motion, scanner], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "motion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sst-rdex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
